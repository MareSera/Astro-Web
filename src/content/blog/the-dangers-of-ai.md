---
title: 'AI可能极其危险->无论它是否有意识'
description: '人工智能'
pubDate: 'Aug 01 2024'
image: '/home.webp'
tags:
  - 人工智能
badge: 人工智能
categories:
  - 技术
---



人工智能正在以惊人的速度进步，我们可能很快就会失去对它的控制，给人类带来巨大的风险。

想象一下：我们不会期望一个新生儿能击败国际象棋大师。那么，我们为什么会期望能够控制一个超级智能的人工智能呢？我们不能简单地关掉它——它已经想到了我们可能会尝试的所有方法，并阻止了我们。

或者考虑一下：一个超级智能的人工智能可以在一秒钟内完成100名人类工程师需要一年才能完成的工作。它大约可以在一秒钟内设计出一架新飞机或武器系统。

“我曾经认为人工智能变得比人类更聪明还很远。不再了，” Geoffrey Hinton 说，他是一位顶级的人工智能科学家，最近离开了谷歌，以警告人工智能的危险。

他并不孤单。2023年的一项调查发现，36%的人工智能专家担心人工智能可能造成“核级别灾难”。包括像史蒂夫·沃兹尼亚克和埃隆·马斯克这样的科技领袖在内的近28,000人签署了一封信，要求暂停高级人工智能开发六个月。

作为一名意识研究者，我也分享这些担忧，并签署了这封信。

有人说这些大型语言模型只是没有意识的花哨机器，所以它们不太可能逃脱。我同意它们可能还没有意识，但这并不重要。一枚核弹可以在没有意识的情况下杀死数百万人。人工智能也可以做到同样的事情，无论是直接（不太可能）还是通过操纵人类（更有可能）。

因此，关于人工智能意识的辩论对于人工智能安全来说并不重要。即使是看似无害的应用，如人工智能男友，如果使用不当，也可能带来风险。

我们为什么这么担心？简单来说：人工智能发展得太快了。

主要问题是新的聊天机器人或“大型语言模型”（LLMs）在对话方面的进步速度有多快。这种快速增长可能很快就会导致“人工通用智能”（AGI），在这种情况下，人工智能可以在没有人类的情况下自我改进。当这种情况发生时，我们可能无法控制它。

这并不是夸张。我们可能只有一次尝试的机会，如果我们搞砸了，我们可能没有机会再试一次。

微软的研究人员测试了OpenAI的GPT-4，他们说它显示出“高级通用智能的火花”。它在律师资格考试上的得分超过了90%的人类，而之前的版本只有10%。他们在许多其他测试中也看到了类似的进步。

这些主要是推理测试。这就是为什么研究人员认为GPT-4可能是AGI的早期版本。

这种快速的变化是为什么Hinton告诉《纽约时报》：“看看五年前的情况，再看看现在。把差异向前推，这是可怕的。” OpenAI的Sam Altman甚至告诉参议院，监管人工智能是“至关重要的”。

当我们把这些人工智能放入机器人时，它们将以同样的超级智能在现实世界中行动，并且能够以惊人的速度复制和改进自己。

我们尝试构建的任何安全措施一旦人工智能变得超级智能，都会被它轻易地识破和禁用。我们将无法控制它们，因为它们会以比我们快得多的速度想到我们可能做的所有事情。它们将突破我们设定的任何限制，就像格列佛逃脱了小人国的细小绳索一样。

一旦人工智能能够自我改进，这可能只是几年后甚至现在，我们不会知道它将做什么或如何控制它。一个超级智能的人工智能可以轻易地智胜人类，操纵我们，并在虚拟和物理世界中行动。

这被称为“控制问题”或“对齐问题”。像Nick Bostrom、Seth Baum和Eliezer Yudkowsky这样的专家已经研究了几十年。

是的，像GPT-4这样的模型已经存在。但人们要求的暂停是停止制造新的、更强大的模型。如果需要，我们可以通过关闭这些模型所需的庞大服务器场来执行这一点。

我认为，创建我们已知在不久的将来无法控制的系统是非常不明智的。我们需要知道何时从边缘退后。现在就是那个时候。

我们不应该比我们已经打开的潘多拉盒子更多地打开它。风险不仅限于基于文本的人工智能，影响从人工智能专辑封面生成工具到更具争议性的领域，如nsfw角色人工智能的各种应用。像nsfw character ai人工智能深度伪造这样的最近事件引发了对更严格政策的呼声，突显了在人工智能发展中需要谨慎考虑的紧迫性。

